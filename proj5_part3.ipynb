{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdc8gjhXpP_2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as skio\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "import scipy\n",
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnZ7qYPVooaW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    !wget https://people.eecs.berkeley.edu/~zhecao/ibug_300W_large_face_landmark_dataset.zip\n",
    "    !unzip 'ibug_300W_large_face_landmark_dataset.zip'    \n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgwciH4B8Rox",
    "outputId": "23a46be5-8e97-40ac-8fb1-4918aa4e17c2"
   },
   "outputs": [],
   "source": [
    "!wget https://people.eecs.berkeley.edu/~zhecao/ibug_300W_large_face_landmark_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtS4stR-8Hhi"
   },
   "outputs": [],
   "source": [
    "!unzip 'ibug_300W_large_face_landmark_dataset.zip'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kdo9WwvAvbM"
   },
   "outputs": [],
   "source": [
    "!rm -r 'ibug_300W_large_face_landmark_dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "xav8jrC94v1W",
    "outputId": "9399498e-22ea-49e9-b532-47dd0e9a3d7f"
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "root = tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "bboxes = [] # face bounding box used to crop the image\n",
    "landmarks = [] # the facial keypoints/landmarks for the whole training dataset\n",
    "img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in root[2]:\n",
    "\timg_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "\tbox = filename[0].attrib\n",
    "\t# x, y for the top left corner of the box, w, h for box width and height\n",
    "\tbboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "\n",
    "\tlandmark = []\n",
    "\tfor num in range(68):\n",
    "\t\tx_coordinate = int(filename[0][num].attrib['x'])\n",
    "\t\ty_coordinate = int(filename[0][num].attrib['y'])\n",
    "\t\tlandmark.append([x_coordinate, y_coordinate])\n",
    "\tlandmarks.append(landmark)\n",
    "\n",
    "landmarks = np.array(landmarks).astype('float32')     \n",
    "bboxes = np.array(bboxes).astype('float32') \n",
    "\n",
    "print(bboxes[5])\n",
    "\n",
    "img = skio.imread(img_filenames[5])\n",
    "print(img.shape)\n",
    "landmark = landmarks[5]\n",
    "bbox = bboxes[5].astype(np.int)\n",
    "plt.imshow(img)\n",
    "plt.plot([bbox[0], bbox[0] + bbox[2], bbox[0] + bbox[2], bbox[0]], [img.shape[0] - bbox[1], bbox[1], bbox[1] + bbox[3], bbox[1] + bbox[3]])\n",
    "plt.plot(landmark[:, 0], landmark[:, 1], linestyle = \"none\", marker = \".\", markersize = 3)\n",
    "cropped_5 = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), bbox[1], bbox[0], bbox[3], bbox[2])\n",
    "print(cropped_5.numpy().shape)\n",
    "skio.imshow(cropped_5.permute((1, 2, 0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAsxCRCg6iuZ"
   },
   "outputs": [],
   "source": [
    "# Adjust the topleft corner and size of the bounding box to let it include as \n",
    "# many feature points as possible. \n",
    "bboxes_adjusted = bboxes.copy().astype(np.int)\n",
    "mask = bboxes_adjusted >= 0\n",
    "bboxes_adjusted[:, 0] = (bboxes_adjusted[:, 0] - bboxes_adjusted[:, 2] * 0.1).astype(np.int)\n",
    "bboxes_adjusted[:, 1] = (bboxes_adjusted[:, 1] - bboxes_adjusted[:, 3] * 0.1).astype(np.int)\n",
    "bboxes_adjusted[:, 2] = (bboxes_adjusted[:, 2] * 1.2).astype(np.int)\n",
    "bboxes_adjusted[:, 3] = (bboxes_adjusted[:, 3] * 1.2).astype(np.int)\n",
    "# print(np.sum(bboxes_adjusted[:, 3] < 0))\n",
    "# img = skio.imread(img_filenames[139])\n",
    "# landmark = landmarks[139]\n",
    "# bbox = bboxes[139].astype(np.int)\n",
    "# print(bboxes[139])\n",
    "# # plt.imshow(img)\n",
    "# # plt.plot([bbox[0], bbox[0] + bbox[2], bbox[0] + bbox[2], bbox[0]], [bbox[1], bbox[1], bbox[1] + bbox[3], bbox[1] + bbox[3]])\n",
    "# # plt.plot(landmark[:, 0], landmark[:, 1], linestyle = \"none\", marker = \".\", markersize = 3)\n",
    "\n",
    "# cropped_139 = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), bbox[1], bbox[0], bbox[3], bbox[2])\n",
    "# print(cropped_139.permute((1, 2, 0)).numpy().shape)\n",
    "# skio.imshow(cropped_139.permute((1, 2, 0)).numpy())\n",
    "# plt.plot(landmark[:, 0] - bbox[0], landmark[:, 1] - bbox[1], linestyle = \"none\", marker = \".\", markersize = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XV7z32sxdYmg"
   },
   "outputs": [],
   "source": [
    "# Adjust landmark pixel location after resizing the cropped image to 224*224. \n",
    "landmarks_adjusted = landmarks.copy()\n",
    "landmarks_adjusted[:, :, 0] = (landmarks_adjusted[:, :, 0] - bboxes_adjusted[:, 0].reshape((bboxes_adjusted.shape[0], 1))) / bboxes_adjusted[:, 2].reshape((bboxes_adjusted.shape[0], 1)) * 224\n",
    "landmarks_adjusted[:, :, 1] = (landmarks_adjusted[:, :, 1] - bboxes_adjusted[:, 1].reshape((bboxes_adjusted.shape[0], 1))) / bboxes_adjusted[:, 3].reshape((bboxes_adjusted.shape[0], 1)) * 224\n",
    "\n",
    "# images = []\n",
    "# for i in range(len(img_filenames)):\n",
    "#   if i % 100 == 0:\n",
    "#     print(i)\n",
    "#   img = skio.imread(img_filenames[i])\n",
    "#   bbox = bboxes_adjusted[i]\n",
    "#   if len(img.shape) == 3:\n",
    "#     print(\"crop image with rgb channel\")\n",
    "#     img = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "#                                                         bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "#     print(img.shape)\n",
    "#   if len(img.shape) == 2:\n",
    "#     print(\"crop black and white image\")\n",
    "#     img = np.dstack((img, img, img))\n",
    "#     img = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "#                                                         bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "#   images.append(resize(img, (224, 224), anti_aliasing=True))\n",
    "  \n",
    "#   if i <= 7:\n",
    "#     plt.imshow(images[i])\n",
    "#     plt.plot(landmarks_adjusted[i][:, 0], landmarks_adjusted[i][:, 1], linestyle = \"none\", marker = \".\")\n",
    "#     plt.show()\n",
    "#   if i > 7:\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FSVn1m0C4wo"
   },
   "outputs": [],
   "source": [
    "num_data = landmarks_adjusted.shape[0]\n",
    "num_training = 6000\n",
    "num_validation = 666\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Training Dataset. \n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "          root_dir (string): Directory with all the images and their feature\n",
    "              locations.\n",
    "          transform (callable, optional): Optional transform to be applied\n",
    "              on a sample.\n",
    "      \"\"\"\n",
    "      # if display:\n",
    "      #     height, width = images[0].shape[:2]\n",
    "      #     for i in range(4):\n",
    "      #         plt.imshow(images[i])\n",
    "      #         plt.plot(feature_points[i][-6, 0] * width,\n",
    "      #             feature_points[i][-6, 1] * height,\n",
    "      #             linestyle = \"none\", marker = \".\")\n",
    "      #         plt.show()\n",
    "\n",
    "      self.feature_points = np.array(landmarks_adjusted[:num_training]).astype(np.float32)\n",
    "      self.images_name = img_filenames[:num_training]\n",
    "      self.bboxes_adjusted = bboxes_adjusted[:num_training]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.feature_points.shape[0]\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    img_name = self.images_name[idx]\n",
    "    img = skio.imread(img_name)\n",
    "    \n",
    "    bbox = self.bboxes_adjusted[idx]\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.dstack((img, img, img))\n",
    "    img = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "                                                bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "    img = skimage.color.rgb2gray(img.numpy())\n",
    "    img = resize(img, (224, 224), anti_aliasing=True).astype(np.float32)\n",
    "    feature = self.feature_points[idx].astype(np.float32)\n",
    "    sample = {'image': img, 'feature': feature}\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKVscGLXefEk"
   },
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "  \"\"\"Validation Dataset.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images and their feature\n",
    "            locations.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "\n",
    "    self.feature_points = np.array(landmarks_adjusted[-num_validation:]).astype(np.float32)\n",
    "    self.images_name = img_filenames[-num_validation:]\n",
    "    self.bboxes_adjusted = bboxes_adjusted[-num_validation:]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.feature_points.shape[0]\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    img_name = self.images_name[idx]\n",
    "    img = skio.imread(img_name)\n",
    "    \n",
    "    bbox = self.bboxes_adjusted[idx]\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.dstack((img, img, img))\n",
    "    img = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "                                                bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "    img = skimage.color.rgb2gray(img.numpy())\n",
    "    img = resize(img, (224, 224), anti_aliasing=True).astype(np.float32)\n",
    "    feature = self.feature_points[idx].astype(np.float32)\n",
    "    sample = {'image': img, 'feature': feature}\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STcipak5iw31"
   },
   "outputs": [],
   "source": [
    "def training_dataloader(batch_size = 8):\n",
    "    \"\"\"\n",
    "    Returns the training dataset and a training dataloader. \n",
    "    \"\"\"\n",
    "    training_data = TrainingDataset()\n",
    "    return training_data, \\\n",
    "        DataLoader(training_data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "def validation_dataloader(batch_size = num_validation):\n",
    "    \"\"\"\n",
    "    Returns the training dataset and a training dataloader. \n",
    "    \"\"\"\n",
    "    validation_data = ValidationDataset()\n",
    "    return validation_data, \\\n",
    "        DataLoader(validation_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C1phw_qJjkYa",
    "outputId": "4dee3af7-2dde-4aae-b7b6-f89e55c2e10a"
   },
   "outputs": [],
   "source": [
    "# Visualize training dataset and corresponding facial keypoints. \n",
    "training_data, training_loader = training_dataloader()\n",
    "validation_data, validation_loader = validation_dataloader()\n",
    "for i, batch in enumerate(training_loader):\n",
    "  images = batch[\"image\"]\n",
    "  features = batch[\"feature\"]\n",
    "  print(features.shape)\n",
    "  print(images.shape)\n",
    "  for j in range(images.shape[0]):\n",
    "    plt.imshow(images[j].numpy())\n",
    "    plt.plot(features[j].numpy()[:, 0], features[j].numpy()[:, 1], linestyle = \"none\", marker = \".\")\n",
    "    plt.show()\n",
    "    if j > 2:\n",
    "      break\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d86HJhMQj17_"
   },
   "outputs": [],
   "source": [
    "def rotate(x, y, angles):\n",
    "    \"\"\"\n",
    "    Rotate the image and find corresponding rotated feature points. \n",
    "    For each angle in angles, rotate all images in batch x with that angle. \n",
    "    \"\"\"\n",
    "    x_rotated_lst = []\n",
    "    y_rotated_lst = []\n",
    "    for angle in angles:\n",
    "        rotation_matrix = np.array([[np.cos(-angle / 180 * np.pi),\n",
    "            np.sin(-angle / 180 * np.pi)],\n",
    "            [-np.sin(-angle / 180 * np.pi),\n",
    "            np.cos(-angle / 180 * np.pi)]]).astype(np.float32)\n",
    "        x_rotated = torchvision.transforms.functional.affine(x, angle = angle,\n",
    "            translate = [0, 0], scale = 1, shear = [0, 0], fill = -0.5)\n",
    "        y_rotated = y.clone()\n",
    "        y_rotated[:, :, 0] = y_rotated[:, :, 0] - width // 2\n",
    "        y_rotated[:, :, 1] = y_rotated[:, :, 1] - height // 2\n",
    "        y_rotated = torch.matmul(y_rotated, torch.from_numpy(rotation_matrix.T))\n",
    "        y_rotated[:, :, 0] = (y_rotated[:, :, 0] + width // 2)\n",
    "        y_rotated[:, :, 1] = (y_rotated[:, :, 1] + height // 2)\n",
    "        x_rotated_lst.append(x_rotated)\n",
    "        y_rotated_lst.append(y_rotated)\n",
    "    return x_rotated_lst, y_rotated_lst\n",
    "\n",
    "def shift_vertical(x, y, pixels):\n",
    "    \"\"\"\n",
    "    Shift the image vertically and find corresponding shifted feature points. \n",
    "    For each pixel value in pixels, shift all images in batch x with that pixel. \n",
    "    \"\"\"\n",
    "    x_shifted_lst = []\n",
    "    y_shifted_lst = []\n",
    "    for pixel in pixels:\n",
    "        x_shifted_vertical = torchvision.transforms.functional.affine(x,\n",
    "            angle = 0, translate = [0, pixel], scale = 1,\n",
    "            shear = [0, 0], fill = -0.5)\n",
    "        y_shifted_vertical = y.clone()\n",
    "        y_shifted_vertical[:, :, 1] = y_shifted_vertical[:, :, 1] + \\\n",
    "            pixel\n",
    "        x_shifted_lst.append(x_shifted_vertical)\n",
    "        y_shifted_lst.append(y_shifted_vertical)\n",
    "    return x_shifted_lst, y_shifted_lst\n",
    "\n",
    "def shift_horizontal(x, y, pixels):\n",
    "    \"\"\"\n",
    "    Shift the image horizontally and find corresponding shifted feature points. \n",
    "    For each pixel value in pixels, shift all images in batch x with that pixel. \n",
    "    \"\"\"\n",
    "    x_shifted_lst = []\n",
    "    y_shifted_lst = []\n",
    "    for pixel in pixels:\n",
    "        x_shifted_horizontal = torchvision.transforms.functional.affine(x,\n",
    "            angle = 0, translate = [pixel, 0], scale = 1,\n",
    "            shear = [0, 0], fill = -0.5)\n",
    "        y_shifted_horizontal = y.clone()\n",
    "        y_shifted_horizontal[:, :, 0] = y_shifted_horizontal[:, :, 0] + \\\n",
    "            pixel\n",
    "        x_shifted_lst.append(x_shifted_horizontal)\n",
    "        y_shifted_lst.append(y_shifted_horizontal)\n",
    "    return x_shifted_lst, y_shifted_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjAO7m36qRXF"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "  \"\"\"\n",
    "  Self-implemented ResNet18. \n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(ResNet18, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 64, 7, 2, (3, 3))\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "    self.conv2_1 = nn.Conv2d(64, 64, 3, (1, 1))\n",
    "    self.bn2_1 = nn.BatchNorm2d(64)\n",
    "    self.conv2_2 = nn.Conv2d(64, 64, 1)\n",
    "    self.bn2_2 = nn.BatchNorm2d(64)\n",
    "    self.conv2_3 = nn.Conv2d(64, 64, 3, (1, 1))\n",
    "    self.bn2_3 = nn.BatchNorm2d(64)\n",
    "    self.conv2_4 = nn.Conv2d(64, 64, 1)\n",
    "    self.bn2_4 = nn.BatchNorm2d(64)\n",
    "\n",
    "    self.conv3_1 = nn.Conv2d(64, 128, 3, 2, (1, 1))\n",
    "    self.bn3_1 = nn.BatchNorm2d(128)\n",
    "    self.conv3_2 = nn.Conv2d(128, 128, 1)\n",
    "    self.bn3_2 = nn.BatchNorm2d(128)\n",
    "    self.conv3_3 = nn.Conv2d(128, 128, 3, (1, 1))\n",
    "    self.bn3_3 = nn.BatchNorm(128)\n",
    "    self.conv3_4 = nn.Conv2d(128, 128, 1)\n",
    "    self.bn3_4 = nn.BatchNorm2d(128)\n",
    "    self.conv3_id = nn.Conv2d(64, 128, 1, 2)\n",
    "    self.bn3_id = nn.BatchNorm2d(128)\n",
    "\n",
    "    self.conv4_1 = nn.Conv2d(128, 256, 3, 2, (1, 1))\n",
    "    self.bn4_1 = nn.BatchNorm2d(256)\n",
    "    self.conv4_2 = nn.Conv2d(256, 256, 1)\n",
    "    self.bn4_2 = nn.BatchNorm2d(256)\n",
    "    self.conv3_3 = nn.Conv2d(256, 256, 3, (1, 1))\n",
    "    self.bn4_3 = nn.BatchNorm(256)\n",
    "    self.conv3_4 = nn.Conv2d(256, 256, 1)\n",
    "    self.bn4_4 = nn.BatchNorm2d(256)\n",
    "    self.conv4_id = nn.Conv2d(128, 256, 1, 2)\n",
    "    self.bn4_id = nn.BatchNorm2d(256)\n",
    "\n",
    "    self.conv5_1 = nn.Conv2d(256, 512, 3, 2, (1, 1))\n",
    "    self.bn5_1 = nn.BatchNorm2d(512)\n",
    "    self.conv5_2 = nn.Conv2d(512, 512, 1)\n",
    "    self.bn5_2 = nn.BatchNorm2d(512)\n",
    "    self.conv5_3 = nn.Conv2d(512, 512, 3, (1, 1))\n",
    "    self.bn5_3 = nn.BatchNorm(512)\n",
    "    self.conv5_4 = nn.Conv2d(512, 512, 1)\n",
    "    self.bn5_4 = nn.BatchNorm2d(512)\n",
    "    self.conv5_id = nn.Conv2d(64, 128, 1, 2)\n",
    "    self.bn5_id = nn.BatchNorm2d(128)\n",
    "\n",
    "    self.fc1 = nn.Linear(512, 136)\n",
    "\n",
    "  def forward(x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, (3, 3), 2, 1)\n",
    "\n",
    "    # conv2 block\n",
    "    identity = x\n",
    "    x = self.conv2_1(x)\n",
    "    x = self.bn2_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2_2(x)\n",
    "    x = sel.bn2_2(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "    identity = x\n",
    "    x = self.conv2_3(x)\n",
    "    x = self.bn2_3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2_4(x)\n",
    "    x = sel.bn2_4(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # conv3 block\n",
    "    identity = self.conv3_id(x)\n",
    "    identity = self.bn3_id(identity)\n",
    "    x = self.conv3_1(x)\n",
    "    x = self.bn3_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv3_2(x)\n",
    "    x = sel.bn3_2(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "    identity = x\n",
    "    x = self.conv3_3(x)\n",
    "    x = self.bn3_3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv3_4(x)\n",
    "    x = sel.bn3_4(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # conv4 block\n",
    "    identity = self.conv4_id(x)\n",
    "    identity = self.bn4_id(identity)\n",
    "    x = self.conv4_1(x)\n",
    "    x = self.bn4_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv4_2(x)\n",
    "    x = sel.bn4_2(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "    identity = x\n",
    "    x = self.conv4_3(x)\n",
    "    x = self.bn4_3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv4_4(x)\n",
    "    x = sel.bn4_4(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # conv5 block\n",
    "    identity = self.conv5_id(x)\n",
    "    identity = self.bn5_id(identity)\n",
    "    x = self.conv5_1(x)\n",
    "    x = self.bn5_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv5_2(x)\n",
    "    x = sel.bn5_2(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "    identity = x\n",
    "    x = self.conv5_3(x)\n",
    "    x = self.bn5_3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv5_4(x)\n",
    "    x = sel.bn5_4(x)\n",
    "    x = x + identity\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "    x = self.fc1(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnWeWNqTWbb7"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Slightly modified ResNet18 from torchvision.models. \n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.resnet18 = torchvision.models.resnet18(pretrained=False)\n",
    "    self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size = 7, stride = 2, padding = (3, 3))\n",
    "    self.resnet18.fc = nn.Linear(512, 136)\n",
    "\n",
    "\n",
    "  def forward(self, images):\n",
    "    features = self.resnet18(images)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kW7auTa1SkNw"
   },
   "outputs": [],
   "source": [
    "# Initialize hyperparameters. \n",
    "batch_size = 64\n",
    "rotation_per_batch = 1\n",
    "shift_vertical_per_batch = 1\n",
    "shift_horizontal_per_batch = 1\n",
    "np.random.seed(1234)\n",
    "\n",
    "training_data, training_loader = training_dataloader(batch_size)\n",
    "validation_data, validation_loader = validation_dataloader(batch_size)\n",
    "\n",
    "# num_epoch = 10\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "\n",
    "height, width = training_data[0][\"image\"].shape[0], \\\n",
    "    training_data[0][\"image\"].shape[1]\n",
    "rotation_angles = np.random.rand(len(training_data) // batch_size + 1,\n",
    "    rotation_per_batch) * 30 - 15\n",
    "pixels_vertical = np.random.randint(-10, 10,\n",
    "    (len(training_data) // batch_size + 1, shift_vertical_per_batch))\n",
    "pixels_horizontal = np.random.randint(-10, 10,\n",
    "    (len(training_data) // batch_size + 1, shift_vertical_per_batch))\n",
    "\n",
    "model = ResNet().to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ui0JB3QCM-Fv",
    "outputId": "f0b4ba96-55d5-47c3-8d2a-64f802739909"
   },
   "outputs": [],
   "source": [
    "# Train on ResNet18 for 10 epochs. \n",
    "num_epoch = 10\n",
    "for epoch in range(num_epoch):\n",
    "    print(f\"Start training epoch {epoch}\")\n",
    "    loss_epoch = []\n",
    "    for i, batch in enumerate(training_loader):\n",
    "        print(\"Start training batch \", i)\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"feature\"]\n",
    "\n",
    "        angles = rotation_angles[i]\n",
    "        x_rotated_lst, y_rotated_lst = rotate(x, y, angles)\n",
    "      \n",
    "        # plt.imshow(x_rotated_lst[0][0].numpy(), cmap = \"gray\")\n",
    "        # print(\"begin plot\")\n",
    "        # print(y_rotated_lst[0][0].numpy()[:, 0])\n",
    "        # print(y_rotated_lst[0][0].numpy()[:, 1])\n",
    "        # plt.plot(y_rotated_lst[0][0].numpy()[:, 0], y_rotated_lst[0][0].numpy()[:, 1],\n",
    "        #     linestyle = \"none\", marker = \".\", markersize = 3, color=\"b\")\n",
    "        # print(\"end plot\")\n",
    "        # plt.show()\n",
    "        # break\n",
    "\n",
    "        pixels = pixels_vertical[i]\n",
    "        x_shifted_vertical_lst, y_shifted_vertical_lst = shift_vertical(x, y, pixels)\n",
    "\n",
    "        # plt.imshow(x_shifted_vertical_lst[0][0].numpy(), cmap = \"gray\")\n",
    "        # print(\"begin plot\")\n",
    "        # plt.plot(y_shifted_vertical_lst[0][0].numpy()[:, 0], y_shifted_vertical_lst[0][0].numpy()[:, 1],\n",
    "        #     linestyle = \"none\", marker = \".\", markersize = 3, color=\"b\")\n",
    "        # print(\"end plot\")\n",
    "        # plt.show()\n",
    "        # break\n",
    "\n",
    "\n",
    "        pixels = pixels_horizontal[i]\n",
    "        x_shifted_horizontal_lst, y_shifted_horizontal_lst = shift_horizontal(x, y, pixels)\n",
    "        \n",
    "        # plt.imshow(x_shifted_horizontal_lst[0][0].numpy(), cmap = \"gray\")\n",
    "        # print(\"begin plot\")\n",
    "        # plt.plot(y_shifted_horizontal_lst[0][0].numpy()[:, 0], y_shifted_horizontal_lst[0][0].numpy()[:, 1],\n",
    "        #     linestyle = \"none\", marker = \".\", markersize = 3, color=\"b\")\n",
    "        # print(\"end plot\")\n",
    "        # plt.show()\n",
    "        # break\n",
    "\n",
    "        x = torch.cat(tuple(x_rotated_lst + x_shifted_vertical_lst + x_shifted_horizontal_lst + [x]),\n",
    "            axis = 0)\n",
    "        y = (torch.cat(tuple(y_rotated_lst + y_shifted_vertical_lst + y_shifted_horizontal_lst + [y]),\n",
    "            axis = 0) / 224).to(\"cuda\")\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        output = model(x.to(\"cuda\"))\n",
    "        loss = criterion(torch.flatten(y, 1), output).float()\n",
    "        loss_epoch.append(loss.item())\n",
    "        print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "          model.eval()\n",
    "          for j, batch in enumerate(validation_loader):\n",
    "            x = batch[\"image\"]\n",
    "            y = batch[\"feature\"]\n",
    "            output = model(x.unsqueeze(1).to(\"cuda\"))\n",
    "            for k in range(4):\n",
    "              actual_index = -num_validation + batch_size * j + k\n",
    "              plt.imshow(skio.imread(img_filenames[actual_index]), cmap = \"gray\")\n",
    "              plt.plot(y[k].numpy()[:, 0] / 224 * bboxes_adjusted[actual_index][2] + bboxes_adjusted[actual_index][0],\n",
    "                y[k].numpy()[:, 1] / 224 * bboxes_adjusted[actual_index][3] + bboxes_adjusted[actual_index][1], \n",
    "                linestyle = \"none\", marker = \".\", markersize = 12, color = 'g')\n",
    "              plt.plot(output[k].detach().cpu().numpy()[::2] * bboxes_adjusted[actual_index][2] + bboxes_adjusted[actual_index][0],\n",
    "                output[k].detach().cpu().numpy()[1::2] * bboxes_adjusted[actual_index][3] + bboxes_adjusted[actual_index][1],\n",
    "                linestyle = \"none\", marker = \".\", markersize = 12, color = 'r')\n",
    "              plt.show()\n",
    "            break\n",
    "          model.train()\n",
    "    training_losses.append(np.mean(loss_epoch))\n",
    "    model.eval()\n",
    "    validation_loss_epoch = 0\n",
    "    for i, batch in enumerate(validation_loader):\n",
    "        validation_x = batch[\"image\"].unsqueeze(1).to(\"cuda\")\n",
    "        validation_y = batch[\"feature\"].to(\"cuda\") / 224\n",
    "        validation_output = model(validation_x)\n",
    "        validation_loss = criterion(torch.flatten(validation_y, 1),\n",
    "            validation_output).float()\n",
    "        validation_loss_epoch += validation_loss.item() * validation_x.shape[0]\n",
    "    validation_loss_epoch = validation_loss_epoch / num_validation\n",
    "    print(validation_loss_epoch)\n",
    "    validation_losses.append(validation_loss_epoch)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4WxmmjCjTOF"
   },
   "outputs": [],
   "source": [
    "# Run this cell if want to save model for future use. \n",
    "torch.save(model, \"resnet18_no_pretrain.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TAlQ4YuubIjl",
    "outputId": "bedda4a4-c448-45ae-f7d7-ac67aef03593"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Visualize prediction on validation set. \n",
    "for i, batch in enumerate(validation_loader):\n",
    "  print(\"Start validating batch \", i)\n",
    "  x = batch[\"image\"]\n",
    "  y = batch[\"feature\"]\n",
    "  output = model(x.unsqueeze(1).to(\"cuda\"))\n",
    "  if i == 1:\n",
    "    for j in range(4):\n",
    "      actual_index = -num_validation + batch_size * i + j\n",
    "      plt.imshow(skio.imread(img_filenames[-num_validation + batch_size * i + j]), cmap = \"gray\")\n",
    "      plt.plot(y[j].numpy()[:, 0] / 224 * bboxes_adjusted[actual_index][2] + bboxes_adjusted[actual_index][0],\n",
    "            y[j].numpy()[:, 1] / 224 * bboxes_adjusted[actual_index][3] + bboxes_adjusted[actual_index][1], \n",
    "            linestyle = \"none\", marker = \".\", markersize = 12, color = 'g')\n",
    "      plt.plot(output[j].detach().cpu().numpy()[::2] * bboxes_adjusted[actual_index][2] + bboxes_adjusted[actual_index][0],\n",
    "            output[i].detach().cpu().numpy()[1::2] * bboxes_adjusted[actual_index][3] + bboxes_adjusted[actual_index][1],\n",
    "            linestyle = \"none\", marker = \".\", markersize = 12, color = 'r')\n",
    "      plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Ju9Wwww7c8hZ",
    "outputId": "f7a50e68-c946-4f41-e325-784f8f38d768"
   },
   "outputs": [],
   "source": [
    "# Plot validation and training losses. \n",
    "plt.plot(np.arange(len(training_losses)), training_losses, label = \"Training loss\")\n",
    "plt.plot(np.arange(len(validation_losses)), validation_losses, label = \"Validation loss\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "AxMraLgHdypS",
    "outputId": "c9ced584-7caf-4f46-ae0d-1d03121b7814"
   },
   "outputs": [],
   "source": [
    "# Test on additional self-uploaded images. \n",
    "img_names = [\"img_1.jpeg\", \"img_2.jpeg\", \"img_3.jpeg\"]\n",
    "bounding_boxes = np.array([[33, 42, 130, 132], [40, 53, 113, 116], [360, 238, 358, 363]])\n",
    "for i in range(len(img_names)):\n",
    "  img = skio.imread(img_names[i])\n",
    "  bbox = bounding_boxes[i]\n",
    "  x = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "                                              bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "  x = skimage.color.rgb2gray(x)\n",
    "  x = torch.from_numpy(resize(x, (224, 224), anti_aliasing=True).astype(np.float32))\n",
    "  output = model(x.unsqueeze(0).unsqueeze(0).to(\"cuda\"))\n",
    "  plt.imshow(img)\n",
    "  plt.plot(output[0].detach().cpu().numpy()[::2] * bbox[2] + bbox[0],\n",
    "          output[0].detach().cpu().numpy()[1::2] * bbox[3] + bbox[1],\n",
    "          linestyle = \"none\", marker = \".\", markersize = 12, color = 'r')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMvR6szdmNaa"
   },
   "outputs": [],
   "source": [
    "test_tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_parsed.xml')\n",
    "test_root = test_tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "test_bboxes = [] # face bounding box used to crop the image\n",
    "test_img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in test_root[2]:\n",
    "  test_img_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "  box = filename[0].attrib\n",
    "  # x, y for the top left corner of the box, w, h for box width and height\n",
    "  test_bboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "  \n",
    "test_bboxes = np.array(test_bboxes).astype('float32') \n",
    "\n",
    "test_bboxes_adjusted = test_bboxes.copy().astype(np.int)\n",
    "test_bboxes_adjusted[:, 0] = test_bboxes_adjusted[:, 0] - test_bboxes_adjusted[:, 2] * 0.1\n",
    "test_bboxes_adjusted[:, 1] = test_bboxes_adjusted[:, 1] - test_bboxes_adjusted[:, 3] * 0.1\n",
    "test_bboxes_adjusted[:, 2] = test_bboxes_adjusted[:, 2] * 1.2\n",
    "test_bboxes_adjusted[:, 3] = test_bboxes_adjusted[:, 3] * 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwlwVva-uzdw",
    "outputId": "9bcc7009-a007-4133-818f-6c73d03bb432"
   },
   "outputs": [],
   "source": [
    "print(test_bboxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_zY3VOCnRkf"
   },
   "outputs": [],
   "source": [
    "num_test = test_bboxes_adjusted.shape[0]\n",
    "class TestDataset(Dataset):\n",
    "  \"\"\"Test Dataset.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images and their feature\n",
    "            locations.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "\n",
    "    self.images_name = test_img_filenames\n",
    "    self.bboxes_adjusted = test_bboxes_adjusted\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.bboxes_adjusted.shape[0]\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    img_name = self.images_name[idx]\n",
    "    img = skio.imread(img_name)\n",
    "    \n",
    "    bbox = self.bboxes_adjusted[idx]\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.dstack((img, img, img))\n",
    "    img = torchvision.transforms.functional.crop(torch.from_numpy(img).permute((2, 0, 1)), \n",
    "                                                bbox[1], bbox[0], bbox[3], bbox[2]).permute((1, 2, 0))\n",
    "    img = skimage.color.rgb2gray(img.numpy())\n",
    "    img = resize(img, (224, 224), anti_aliasing=True).astype(np.float32)\n",
    "    sample = {'image': img}\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyoxLnVNn_G1"
   },
   "outputs": [],
   "source": [
    "def test_dataloader(batch_size = num_test):\n",
    "    \"\"\"\n",
    "    Returns a test dataset and test dataloader. \n",
    "    \"\"\"\n",
    "    test_data = TestDataset()\n",
    "    return test_data, \\\n",
    "        DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cgdn0LXJoQ6W",
    "outputId": "20430f79-0147-4074-8183-de28f0550e53"
   },
   "outputs": [],
   "source": [
    "# Visualize predictions on test dataset. \n",
    "model.eval()\n",
    "\n",
    "test_batch_size = 64\n",
    "\n",
    "test_data, test_loader = test_dataloader(test_batch_size)\n",
    "result = torch.Tensor()\n",
    "\n",
    "for i, batch in enumerate(test_loader):\n",
    "  print(\"Start testing batch \", i)\n",
    "  x = batch[\"image\"]\n",
    "  output = model(x.unsqueeze(1).to(\"cuda\"))\n",
    "  actual_x_index = output.detach().cpu().numpy()[:, ::2] * \\\n",
    "    test_bboxes_adjusted[i * test_batch_size:min((i + 1) * test_batch_size, num_test), [2]] + \\\n",
    "    test_bboxes_adjusted[i * test_batch_size:min((i + 1) * test_batch_size, num_test), [0]]\n",
    "  actual_y_index = output.detach().cpu().numpy()[:, 1::2] * \\\n",
    "    test_bboxes_adjusted[i * test_batch_size:min((i + 1) * test_batch_size, num_test), [3]] + \\\n",
    "    test_bboxes_adjusted[i * test_batch_size:min((i + 1) * test_batch_size, num_test), [1]]\n",
    "  if i == 1:\n",
    "    for j in range(40,44):\n",
    "      actual_index = test_batch_size * i + j\n",
    "      plt.imshow(skio.imread(test_img_filenames[actual_index]), cmap = \"gray\")\n",
    "      plt.plot(actual_x_index[j], actual_y_index[j],\n",
    "            linestyle = \"none\", marker = \".\", markersize = 12, color = 'r')\n",
    "      plt.show()\n",
    "  result = torch.cat((result, \n",
    "                     torch.from_numpy(np.array([actual_x_index, actual_y_index])).permute(1, 2, 0).flatten(1)))\n",
    "  print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "id": "x0N9AWA2wqNq",
    "outputId": "371ec732-c486-41b2-ae7d-5b2b47f986f4"
   },
   "outputs": [],
   "source": [
    "# Flatten results to store in .csv file. \n",
    "result = result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfiahoyqHwNY"
   },
   "outputs": [],
   "source": [
    "# Save predictions to .csv file. Submit to Kaggle afterwards. \n",
    "df = pd.DataFrame({\"Id\":np.arange(len(result)), \"Predicted\": result.cpu().numpy()})\n",
    "df.to_csv(\"results_catherine_gai.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVuw40ZvOZwX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "proj5_part3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
